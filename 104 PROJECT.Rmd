---
title: "Econ 104 Course Project"
author: "Valeria Meza, Brianna Pineda, Sara Hameed"
date: "2026-02-11"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(Boruta)
library(car)
library(lmtest)
library(sandwich)
library(tseries)
library(boot)
library(caret)

set.seed(123)
```

```{r load-data}
df <- read_csv("data/msa_housing_final.csv")
nrow(df)
glimpse(df)
```

```{r create-employment-rate}
# Create employment_rate if missing (so you have 10 predictors)
if (!"employment_rate" %in% names(df)) {
  df <- df %>% mutate(employment_rate = 1 - unemployment_rate)
}

"employment_rate" %in% names(df)
```


```{r define-vars}
y_var <- "med_home_value"

quant_vars <- c(
  "med_income",
  "log_income",
  "population",
  "log_population",
  "unemployment_rate",
  "employment_rate",
  "med_age",
  "age_squared",
  "vacancy_rate",
  "college_share"
)

cat("Missing variables:", setdiff(c(y_var, quant_vars), names(df)), "\n")
```
## Introduction
(put your intro paragraph here)

## 1. Economic Theory and Hypothesis
### (a) Research Question
### (b) Economic Theory
     Economic theory suggests that constrained metropolitan housing supply causes higher housing prices. The elasticity of housing supply plays a key role in determining the price of housing. In a traditional supply and demand model, supply adjusts to demand changes based on its elasticity. With elastic supply, production races to meet increased demand. In metropolitan housing markets, however, housing supply is rather inelastic. As city populations grow, demand for housing follows suit. When the demand for more housing in big cities increases, barriers such as zoning laws, environmental regulations, and a lack of available land make it hard to expand. In the long run, acquiring permits, financing, and construction are often multi-year endeavors, adding to the inability of housing supply to quickly meet new demand. Supply of housing is inelastic and unable to change very much in response to demand shocks like migration and income growth. In condensed cities, housing supply is especially inelastic because of limited space for construction (Saiz, 2010). Rather than increasing the number of homes through construction, house values inflate to reach equilibrium. Therefore, metropolitan areas with larger populations tend to exhibit higher home values.   
     Housing prices depend strongly on local labor market conditions. Shocks to the labor market can act as a strong influence on the demand for housing in urban areas. When there’s a negative shock to the job market, in the form of rising unemployment, we can expect a negative effect on housing prices. The demand for housing is dependent on consumer income, and when there’s a negative shock to the labor market, reducing employment and income, the housing demand curve shifts inward. Unemployment leads to fewer qualified mortgage applicants, effectively driving the buyer market for housing down. Less job stability and more income insecurity bode poorly for borrowers seeking to secure loans to purchase property (Mian and Sufi, 2011).  Further, when unemployed people can’t find work in the city, they may be tempted to migrate toward more fruitful labor markets, reducing local housing demand (Blanchard and Katz, 1992). When purchasing power and growth opportunities are limited, property purchases plummet, driving down prices. Therefore, metropolitan areas with higher unemployment rates yield lower median house prices. 
     In the long run, persistent differences in workforce education levels play a role in creating differences in housing demand across metropolitan regions. Economic theory suggests that high-skill workers or those with higher education attainment gravitate toward productive metropolises with higher wages and more employment opportunities, even when housing is more expensive (Diamond, 2016). This sorting pattern creates high-skill clusters in cities with firms that require such workers. Those within these clusters earn higher income and increased purchasing power, so their willingness to pay for more expensive homes rises. With a limited housing supply and high demand for property, competitor buyers bid up the values of homes. As a result, these metropolises with clusters of highly educated workers sustain higher median housing prices.

### (c) Hypotheses
### (d) Expected Signs



## 2. Variable Selection 
### (a) Boruta Algorithm (Quantitative Predictors)  
```{r boruta}
boruta_df <- df %>%
  select(all_of(c(y_var, quant_vars))) %>%
  drop_na()

X <- boruta_df %>% select(all_of(quant_vars))
y <- boruta_df[[y_var]]

set.seed(123)
boruta_fit <- Boruta(x = X, y = y, doTrace = 1, maxRuns = 100)
boruta_fixed <- TentativeRoughFix(boruta_fit)

confirmed <- getSelectedAttributes(boruta_fixed, withTentative = FALSE)
confirmed

plot(boruta_fixed, las = 2, cex.axis = 0.7)
```
We applied the Boruta algorithm to identify the most relevant quantitative predictors of median home value across U.S. metropolitan statistical areas. The algorithm compares the importance of each predictor to randomized shadow variables and retains only those that consistently outperform the shadows. Using 100 iterations and a fixed random seed for reproducibility, Boruta confirmed all ten candidate quantitative predictors as important. These include measures of income, population size, labor market conditions, demographic structure, housing market tightness, and educational attainment.

### (b) Factor/indicator variables
```{r create-indicators}

# -----------------------------
# Create Indicator Variables
# -----------------------------

# 1. High-income MSA (above median income)
income_cutoff <- median(df$med_income, na.rm = TRUE)

df <- df %>%
  mutate(
    high_income_msa = ifelse(med_income > income_cutoff, 1, 0)
  )

table(df$high_income_msa)


# 2. Large MSA (above median population)
population_cutoff <- median(df$population, na.rm = TRUE)

df <- df %>%
  mutate(
    large_msa = ifelse(population > population_cutoff, 1, 0)
  )

table(df$large_msa)


# 3. Tight housing market (below median vacancy rate)
vacancy_cutoff <- median(df$vacancy_rate, na.rm = TRUE)

df <- df %>%
  mutate(
    tight_market = ifelse(vacancy_rate < vacancy_cutoff, 1, 0)
  )

table(df$tight_market)
```

```{r define-factor-vars}

factor_vars <- c(
  "high_income_msa",
  "large_msa",
  "tight_market"
)

factor_vars
```
The dataset did not contain existing categorical predictors beyond the MSA name. Therefore, we constructed economically meaningful binary indicators. We created a high-income MSA indicator (above the median income), a large metropolitan area indicator (above median population), and a tight housing market indicator (below median vacancy rate). These variables allow us to test whether structural differences across metropolitan areas are associated with variation in housing prices. 

### (c) Final predictor set
```{r final-predictor-set}

final_predictors <- c(quant_vars, factor_vars)

final_predictors
```

## 3. Descriptive Analysis and EDA
###(a) Provide detailed description of your dataset: source, time period, sample selection, vari- able definitions 
We are analyzing the cross-sectional variation in housing prices across various U.S. Metropolitan Statistical Areas (MSAs) using the American Community Survey (ACS) 5-Year Estimates for 2019-2023. The unit we are observing is the MSA and the final sample includes 393 MSAs. There are no missing values for the outcome variable. The dependent variable is the median home value (med_home_value). We are using the explanatory variables to capture local economic conditions, market size, housing market tightness, and demographic/human capital composition. A few variables are transformed to explain non-linearity and scale differences. 

Variable Definitions on MSA level: 
-med_home_value: median value of owner-occupied housing units (outcome) 
-med_income: median household income 
-population: total population 
-unemployment_rate: unemployed / labor_force 
-employment_rate: 1 − unemployment_rate (constructed) 
-vacancy_rate: vacant_units / housing_units 
-college_share: college_grad / college_total 
-med_age: median age 
-log_income: ln(med_income)
-log_population: ln(population) 
-age_squared: med_age² 

###(b) Begin by providing a descriptive analysis of your variables. This should include things like histograms, quantile plots, correlation plots, boxplots, scatterplots, etc.
```{r eda-vars}
# Outcome + predictors you decided to keep
eda_vars <- c(
  y_var,
  "med_income","log_income",
  "population","log_population",
  "unemployment_rate","employment_rate",
  "med_age","age_squared",
  "vacancy_rate","college_share"
)

# Keep only numeric columns (should be all of them here)
eda_vars <- eda_vars[eda_vars %in% names(df)]
eda_vars
```
###STEP 1 — Histograms (univariate)
```{r eda-histograms}
df_long <- df %>%
  dplyr::select(all_of(eda_vars)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

ggplot(df_long, aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(~variable, scales = "free") +
  labs(title = "Histograms of Outcome and Predictors")
```

###STEP 2 — Quantile plots (QQ plots)
```{r eda-qqplots}
ggplot(df_long, aes(sample = value)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~variable, scales = "free") +
  labs(title = "QQ Plots (Normality Check)")
```

###STEP 3 — Correlation plot (multivariate)
```{r eda-correlation}
num_df <- df %>% dplyr::select(all_of(eda_vars)) %>% drop_na()

cor_mat <- cor(num_df)
round(cor_mat, 3)
```
```{r eda-corr-plot}
GGally::ggcorr(num_df, label = TRUE, label_round = 2) +
  labs(title = "Correlation Plot (Numeric Variables)")
```

### STEP 4 — Boxplots
```{r eda-boxplots}
ggplot(df_long, aes(x = variable, y = value)) +
  geom_boxplot() +
  coord_flip() +
  labs(title = "Boxplots (Outliers / Spread Check)")
```
### STEP 5 — Scatterplots vs the outcome (med_home_value)
```{r eda-scatter-outcome}
predictors <- setdiff(eda_vars, y_var)

for (v in predictors) {
  p <- ggplot(df, aes(x = .data[[v]], y = .data[[y_var]])) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", se = FALSE) +
    labs(title = paste("Scatter:", y_var, "vs", v),
         x = v, y = y_var)
  print(p)
}
```
-Histograms depict the distribution shape and show if logs help. 
-QQ Plots check for normality and flag extreme tails and outliers.
-Correlation Plots highlight multicollinearity. 
-Boxplots show outliers and variables that are dispersed in an unusual way. 
-Scatterplots vs. med_home_value shows the direction and possible nonlinear patterns that motivate transformations. 

###(c) Create a comprehensive descriptive statistics table (mean, median, std dev, min, max) 
```{r eda-descriptive-table}
# Variables to summarize (outcome + predictors)
eda_vars <- c(
  "med_home_value",
  "med_income", "log_income",
  "population", "log_population",
  "unemployment_rate", "employment_rate",
  "med_age", "age_squared",
  "vacancy_rate",
  "college_share"
)

# Descriptive stats table
desc_table <- df %>%
  dplyr::select(dplyr::all_of(eda_vars)) %>%
  summarise(across(everything(),
    list(
      mean = ~mean(.x, na.rm = TRUE),
      median = ~median(.x, na.rm = TRUE),
      sd = ~sd(.x, na.rm = TRUE),
      min = ~min(.x, na.rm = TRUE),
      max = ~max(.x, na.rm = TRUE),
      n = ~sum(!is.na(.x))
    ),
    .names = "{.col}_{.fn}"
  ))

desc_table
```
```{r eda-descriptive-table-pretty}
library(tidyr)

desc_pretty <- desc_table %>%
  pivot_longer(everything(),
               names_to = c("variable","stat"),
               names_sep = "_(?=[^_]+$)",
               values_to = "value") %>%
  pivot_wider(names_from = stat, values_from = value) %>%
  arrange(variable)

desc_pretty
```

###(d) Estimate density plots for all your variables, and show the respective fitted distributions 
### 1) Density plots for all variables
```{r eda-density-all}
df_long <- df %>%
  dplyr::select(dplyr::all_of(eda_vars)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

ggplot(df_long, aes(x = value)) +
  geom_density() +
  facet_wrap(~variable, scales = "free", ncol = 3) +
  labs(title = "Density plots: outcome + predictors")
```

### 2) Fit disttributions 
```{r eda-fit-distributions, message=FALSE, warning=FALSE}
# If this errors, run once in Console: install.packages("fitdistrplus")
library(fitdistrplus)

# Clean vectors: numeric, finite, positive (needed for lognormal)
fit_home <- df$med_home_value
fit_home <- as.numeric(fit_home)
fit_home <- fit_home[is.finite(fit_home) & fit_home > 0]

fit_pop <- df$population
fit_pop <- as.numeric(fit_pop)
fit_pop <- fit_pop[is.finite(fit_pop) & fit_pop > 0]

# Fit distributions
fit_norm_home <- fitdist(fit_home, "norm")
fit_logn_home <- fitdist(fit_home, "lnorm")

fit_norm_pop  <- fitdist(fit_pop, "norm")
fit_logn_pop  <- fitdist(fit_pop, "lnorm")

# Print summaries (so you see output even if plots are weird)
summary(fit_norm_home)
summary(fit_logn_home)
summary(fit_norm_pop)
summary(fit_logn_pop)

# Better plotting function (more reliable than plot(fitdist) in Rmd)
par(mfrow = c(2,2))

denscomp(list(fit_norm_home, fit_logn_home),
         legendtext = c("Normal", "Lognormal"),
         main = "med_home_value: density fit")

qqcomp(list(fit_norm_home, fit_logn_home),
       legendtext = c("Normal", "Lognormal"),
       main = "med_home_value: QQ fit")

denscomp(list(fit_norm_pop, fit_logn_pop),
         legendtext = c("Normal", "Lognormal"),
         main = "population: density fit")

qqcomp(list(fit_norm_pop, fit_logn_pop),
       legendtext = c("Normal", "Lognormal"),
       main = "population: QQ fit")

par(mfrow = c(1,1))
```
The population and home values appear right-skewed. 

###(e) Identify if there are any non-linearities within your variables. What transformations should you perform to make them linear? What would happen if you included nonlinear variables in your regression models without transforming them first? 
We’ll test nonlinearity by:
1) scatterplots of outcome vs predictors
2) adding a LOESS smooth line
3) using log transforms where needed (you already have log_income/log_population and age_squared)

### 1) Scatterplots with LOESS
```{r eda-nonlinearity-scatter}
predictors_to_check <- c(
  "med_income", "population",
  "unemployment_rate", "vacancy_rate",
  "med_age", "college_share"
)

df_scatter <- df %>%
  dplyr::select(med_home_value, dplyr::all_of(predictors_to_check)) %>%
  drop_na()

df_long_scatter <- df_scatter %>%
  pivot_longer(cols = -med_home_value, names_to = "xvar", values_to = "x")

ggplot(df_long_scatter, aes(x = x, y = med_home_value)) +
  geom_point(alpha = 0.4) +
  geom_smooth(method = "loess", se = FALSE) +
  facet_wrap(~xvar, scales = "free", ncol = 2) +
  labs(title = "Nonlinearity check: Outcome vs predictors")
```
Many of the relationships depicted are nonlinear, especially for population and income, due to the right-skewness and diminishing marginal effects. That is why we use log transformations for population and income and allow curvature with age_squared. 

###(f) Comment on any outliers and/or unusual features of your variables, and then justify their removal, exclusion, or imputation
### 1) Outlier counts by variable (IQR rule) 
```{r eda-outliers-iqr}
iqr_outlier_counts <- function(x) {
  q1 <- quantile(x, 0.25, na.rm = TRUE)
  q3 <- quantile(x, 0.75, na.rm = TRUE)
  iqr <- q3 - q1
  lower <- q1 - 1.5*iqr
  upper <- q3 + 1.5*iqr
  sum(x < lower | x > upper, na.rm = TRUE)
}

outlier_summary <- tibble(
  variable = eda_vars,
  outlier_count = sapply(df[eda_vars], iqr_outlier_counts)
)

outlier_summary %>% arrange(desc(outlier_count))
```

### 2) Biggest outliers for population + med_home_value
```{r eda-outliers-top}
df %>%
  dplyr::select(NAME, med_home_value, population, vacancy_rate, med_income) %>%
  arrange(desc(population)) %>%
  slice(1:10)
```
Population and home vales showcase extreme observations that correspond to very large metro areas lije L.A. and NYC. They are not data errors, they are real-world variation, so we retain the values. To reduce their impact and improve linearity, we use log transformations (log_population and log_income) and use robust standard errors later on. 

###(g) If you have any NAs, remove them or impute them using any of the methods discussed in class, but make sure to justify your choice
### 1) Missingness table 
```{r eda-missingness}
na_table <- tibble(
  variable = names(df),
  na_count = sapply(df, function(x) sum(is.na(x))),
  na_share = sapply(df, function(x) mean(is.na(x)))
) %>% arrange(desc(na_count))

na_table
```
```{r check-missing}
colSums(is.na(df))
```
There are no missing values detected in the data set. Therefore, there is no deletion of values needed and all observations were retained for analysis.  

## 4. Model Building and Selection
## 5. Diagnostics and Robustness
## 6. Results and Economic Interpretation
## 7. Limitations and Extensions
## 8. Code Quality and Reproducibility




